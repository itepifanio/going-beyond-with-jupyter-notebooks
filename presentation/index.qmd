---
title: "Going beyond with Jupyter notebooks"
subtitle: "Welcome! This tutorial will present how to use Jupyter notebooks to publish python packages and more"
format:
  revealjs: 
    slide-number: true
    logo: images/palaimon_logo.svg
    css: styles.css
    footer: Going beyond with Jupyter notebooks
---

## Hello there {.smaller}

This tutorial will:

- Introduce the main concepts to publish a package and why to do it
- Discuss the Python packaging system and distribution
- Explain the literate programming concept
- Show Jupyter notebook and Jupyter lab UI
- Introduce Nbdev lib and how we can use literate programming to publish Python packages
- Explain good practices in notebooks and Nbdev
- Develop a French Deck lib and mini game

## Hello there {.smaller}

- Display quality assurance tools
- Discuss notebook testing 
- Show how and where to publish a Python package
- Introduce Github Actions usage add a CI/CD to our workflow
- Talk about Python packaging future and past
- Introduce how to build UI for notebooks
- Show Quarto customization
- Display how to use Chat GPT in notebooks

# Python packaging

Let's introduce/review some core concepts to understand python packaging and its structure

## Modules

> Modules are files that contains Python definitions and statements,
 [Python Docs](https://docs.python.org/3/tutorial/modules.html#modules)

Any file `.py` can be considered a module

```python
%%writefile module.py

def hello(name):
    print(f'hello, {name}')
```

```python
import module

module.hello("audience!")
```

## Modules

Modules can also be executed as python scripts

```python
%%writefile module.py

def hello(name):
    print(f'hello, {name}')

if __name__ == "__main__":
    import sys
    hello(sys.argv[1])
```

```python
! python module.py 'Maria'
```

::: {.callout-note}
The conditional `if __name__ == "__main__"` makes sure that the function `hello` 
it will be executed only when the module it's running as "main" file. This strategy 
allows python files to be imported and also used as scripts
:::

## Python modules search

Python interpreter searchs for a module:

- Looking first at its built-in modules (listed at `sys.builtin_module_names`)
- Looks for python files in the directory list (`sys.path`)

## Packages {.smaller}

> Is a way to struct Python namespaces using dotted module names,
 [Python Docs](https://docs.python.org/3/tutorial/modules.html#packages)

Packages can be understood as a collection of modules. Check the following structure.

```
somefolder/
    {username}_package/
        __init__.py
        module1.py
        subpackage/
            module2.py
```

::: {.callout-note}
- Create this folder structure on your machine
- Open your terminal
- Import module1 `from {username}_package import module1`
- Import module2 `from {username}_package.subpackage import module2`
- Access a different folder (`cd ..`) and import the {username}_package
- sys.path and `pip install -e .[dev]`
:::

## Python package search

When importing packages, Python looks for the directories names listed at `sys.path`

# Why packaging?

Let's discuss the pros and cons of packaging

## Why packaging?

Let's think about the "Avengers" movie as a big package with multiple heroes (Hulk, Black Widow, Thor, etc).

```{mermaid}
flowchart LR
  subgraph avengers1
    A["Avengers\n (Hulk, Black Widow, Thor)"]
  end

  subgraph avengers2
   direction TB
    B["Hulk"] --> C["Avengers"]
    D["Thor"] --> C
    E["Black Widow"] --> C
  end

  avengers1 --> avengers2
```

:::{.callout-note}
Example from [Arquitetura Modular com pacotes Python](https://www.youtube.com/watch?v=Ccw9Dlw9b2c) talk presented at Python Brazil, 2022
:::

:::{.notes}
To reutilize different heroes in different timeline it would be required code duplicated and increased complexity. The idea of creating a package for every hero allows combine them in different movies
:::

## Pros and cons

::: {.columns}

::: {.column width="50%"}
- Increases the code reuse
- Reduces coupling 
- Easier to other developers to use
- Split responsability across teams
- It can make the maintenance easier
:::

::: {.column width="50%"}
- Harder to maintain
- Dependency management can be difficult
- Security of third-party packages
- Packages sometimes have more features than it's required
:::

:::

## How to publish? {.smaller}

We have previously installed our {username} package locally but how we can 
publish on pip and use it to download?

```{mermaid}
flowchart TD
  A["Package project files (setup.py, pyproject.toml)"] --> B["Build backend (sdist, wheel)"]
  B --> C["Package artifact (.tar.gz, .whl)"]
  C --> D["Twine upload (Pypi, TestPypi)"]
```

# Pypi

Python Package Index

## Tooling

- [pip](https://packaging.python.org/en/latest/key_projects/#pip) is the most popular package manager in python
- [PyPI](https://pypi.org/) is the index where pip downloads content from
- [Test PyPI](https://packaging.python.org/en/latest/guides/using-testpypi/) a separated index for testing

:::{.callout-note}
Create your account at https://test.pypi.org, let's use it to publish our {username} package
:::

## Publishing {username} package {.smaller}

- Go to the root (`somefolder`) of the "{username} package"
- Create a setup.py file with the following content:

```python
from setuptools import setup

setup(
    name='{username}_package',
    version='0.0.1',
    packages=['{username}_package']
)
```

**Legacy way**

- Run `python setup.py sdist` and `python setup.py bdist_wheel`
- Check the `dist` folder

## Publishing {username} package {.smaller}

**Current**

- Run `python -m build` (you may need to download `pip install build`)
- Check the `dist` folder
- Check if twine is available by running `twine` in your terminal, otherwise run `pip install twine`
- (Make sure your Test PyPI email is verified)
- Run `twine upload --repository-url https://test.pypi.org/legacy/ dist/*`


## Other packaging repositories

There are others repositories than PyPI and Test PyPI

- [Pypiserver](https://github.com/pypiserver/pypiserver) local or self-hosted
- [JFrog](https://www.jfrog.com/confluence/display/JFROG/PyPI+Repositories) self-host or cloud, complete cloud solution
- [Code artifacts](https://docs.aws.amazon.com/codeartifact/latest/ug/welcome.html) AWS cloud repository
- [Artifact registry](https://cloud.google.com/artifact-registry/docs/python/store-python) Google cloud repository
- [Gitlab package registry](https://docs.gitlab.com/ee/user/packages/pypi_repository/) Gitlab cloud repository, excelent to use in private projects

# Literate programming

Programming paradigm to tell a history with your code

## Literate programming {.smaller}

:::: {.columns} 

::: {.column width="30%"}
- Programming paradigm
- Haskell `.lhs` vs `.hs`
- Grew in the last years
- Data analysis
- Jupyter notebooks, R Studio
:::

::: {.column width="70%"}
![](images/donald-knuth-quote.png)
:::

::::

## Literate programming

```{mermaid}
flowchart
  A["WEB (language and associate programs/system)"] -- "WEAVE (processor)" --> B["TEX"]
  A -- "TANGLE (processor)" --> C["PAS"]
  C -- PASCAL --> D["REL (binary file that can be executed)"]
  B -- LATEX --> E["DVI (Device-independent binary)"]
```

The process of "compile, load, and go" has been slightly lengthened to "tangle, compile, load, and go."

## Literate programming

```{mermaid}
flowchart
  A["Jupyter Notebook"] -- "Quarto" --> B["MD"]
  A -- "Nbdev" --> C["PY"]
  B -- PANDOC --> E["PDF/HTML/DOCX"]
```

## Jupyter notebook introduction {.smaller}

- Using the virtualenv created at {username}_package, install jupyter notebooks: `pip install notebook`
- Run `jupyter-notebook`
- Get familiar with the UI

::: {.callout-tip collapse="true"}
## UI

- [Jupyter Lab](https://jupyterlab.readthedocs.io/en/stable/) it's one of the most popular UI
- Run `pip install jupyterlab` and execute `jupyter-lab` to see its interface

There are additional UIs, like the VSCode plugin that renders notebooks, but I recommend keep using classic notebooks in this tutorial
:::

# Nbdev

Using Jupyter notebooks to publish Python packaging

## Install {.smaller}

- Let's ignore our {username}_package project and fork the following repository: https://github.com/itepifanio/indo-alem-com-jupyter-notebook
- Create a new virtualenv `python -m venv tutorial`
- Activate your new virtualenv `source ./tutorial/bin/activate`
- Install the package running `pip install -e .[dev]`

:::{.callout-note}
Let's get familiar with the nbdev structure
:::

## Post installation

- nbdev_install_hooks
- nbdev_install_quarto

:::{.callout-tip}
Run `nbdev_help` in your terminal and check all commands available
:::

## Nbdev awesome projects {.smaller}

- [FastAI](https://github.com/fastai/fastai)
- [Ipyannotator](https://github.com/palaimon/ipyannotator)
- [TSAI](https://github.com/timeseriesAI/tsai)
- [FastKafta](https://github.com/airtai/fastkafka)
- [Number Blog](https://github.com/crowdcent/numerblox)
- [UPIT](https://github.com/tmabraham/UPIT)
- [AskAI](https://github.com/radekosmulski/ask_ai)
- [Streamlit Jupyter](https://github.com/ddobrinskiy/streamlit-jupyter)
- [Banet](https://github.com/mnpinto/banet)

# Good practices

Jupyter notebook and nbdev best practices

## Notebook type

![Know the type of notebook you're writing](https://documentation.divio.com/_images/overview.png)

## Good title and subtitle

There are two ways of doing that, one with markdown:

```bash
# My H1 title

> My description
```

Other using Quarto:

```bash
---
title: "My title"
description: "My description"
---
```

## Change the text according to the notebook type

- Reference: Start with a small description of the component, use symbolic links for easy navigation
- Tutorials and guides: Describe what the reader will learn and how. Be objective
- Explation: A brief review about the topic is enough to guide the reader

## Use visualizations {.smaller}

Jupyter notebooks are very interactive. Use images, videos, audio

```{python}
import matplotlib.pyplot as plt
plt.plot([1,23,2,4])
plt.plot([8,65,23,90])
plt.show()
```

## Keep your cells small

- No rules for size but take it easy
- Use @patch `from fastcore.basics import patch`
- Increase the testability by writing small function/classes per cell

## Jupyter notebooks

- Keep your imports at the top of the notebook
- Don't import and use code at the same cell
- Avoid ambigous execution orders
- Use code cells for experiments

## Doc parameters {.smaller}

Nbdev have two types of parameter documentation, the classic (numpy-style), as it follows:

```python
def add_np(a, b=0):
    """The sum of two numbers.

    Parameters
    ----------
    a : int
        the 1st number to add
    b : int
        the 2nd number to add (default: 0)
    """
    return a + b
```

## Doc parameters {.smaller}

```{python}
from nbdev.showdoc import show_doc

def add_np(a, b=0):
    """The sum of two numbers.

    Parameters
    ----------
    a : int
        the 1st number to add
    b : int
        the 2nd number to add (default: 0)
    """
    return a + b

show_doc(add_np)
```

## Doc parameters {.smaller}

And the nbdev approach called "docments"

```python
def add(
    a: int,  # the 1st number to add
    b=0,  # the 2nd number to add
):
    "The sum of two numbers."
    return a + b
```

```{python}
from nbdev.showdoc import show_doc

def add(
    a: int,  # the 1st number to add
    b=0,  # the 2nd number to add
):
    "The sum of two numbers."
    return a + b

show_doc(add)
```

## Write tests

- Keyword assert can be used for testing
- Run `nbdev_test`

```python
assert add(1, 1) == 2
```

## Exercice {.smaller}

```python
import collections

Card = collections.namedtuple("Card", ["rank", "suit"])
```

:::{.callout-note}
- Create a new notebook called `card_utils.ipynb`
- Add a title and description to the notebook
- Copy the `Card` class definition
- Create a function `card_name` that returns `1 of Hearts`
- Test that the `card_name` function returns the expected results
- Add your notebook to `sidebar.yml`
- Run `nbdev_preview` and see the changes
- Run `nbdev_test` and check if the testing are passing
:::

# My Deck package

We're already familiar with the `Card` defition. Let's investigate the MyDeck code.

## Exercice

:::{.callout-note}
- Add a code cell at the top of the `card_utils.ipynb` nb with `#| default_exp card_utils`
- Next, add a code cell with `from nbdev import nbdev_export`
- At the end of `card_utils.ipynb` add `nbdev_export()`
- Execute the nb and check the `mydeck` folder
:::

# Dependencies

Avoid dependency hell for others

## Dependency Hell

- When a software grows more libs tend to be added
- Packages are awesome they can add features, avoid/fix errors and provide more security to a software
- But keeping packages updates can be tricky
- Ex. Dependency 1 expects Python 3.7 usage and Dependency 2 expects Python 3.9

## Specify your dependencies

- ~= to specify releases
- == to fix a version
- != to exclude a version
- <=, >= to inclusively specify a range of versions
- <, > to specify a range of versions

## Releasing

- Semantic versioning is a set of rules that helps avoiding dependency hell
- It MUST declara a public API and it SHOULD be comprehensive 
- The version number MUST use the X.Y.Z format where X, Y, Z are 
non-negative integers and each element SHOULD increase numerically
- MAJOR.MINOR.PATCH

## Keep a changelog {.smaller}

- Semantic versioning is very didatic but it's not human readable
- Changelog keeps a chronologicall order list of notable changes of 
each version of a project
- It improve the transparency
- Help to keep developers accountable
- Help stakeholders to understand the project direction
- Keeps a list of the bugs

# Quality Assurance

Improve your code using automatic tools

## NbQA

There are several tools that analises code and:

- Avoid errors and code smells
- Define a code style
- Improve readability
- Measure code quality

## NbQA

Popular tools that analise code:

- Autopep8
- Black
- Flake8
- MyPy
- Isort

All of them (and some others) can be executed using NbQA

```bash
nbqa <tool-name> <tool-params>
```

## MyPy {.smaller}

> Static type checker. It validates your code typing without executing it

Previous to Python 3.5 (we didn't have typing), it was common the docstring usage to define types:

```python
def natural_sum(x, y):
    """
    Args:
        x (int): first parameter of the sum
        y (int): second parameter of the sum
    Returns:
        Optional[int]: Sum of x and y if both bigger than zero
    """
    if x < 0 or y < 0:
        return None

    return x + y
```

## MyPy {.smaller}

The need of tools to validate data typing made Python add the typing system to its core. 

Run the following snippet on your terminal:

```bash
! nbqa mypy *.ipynb --ignore-missing-imports
```

:::{.callout-note}
The type system was developed to help other tools like MyPy to check types 
statically but libs like pydantic started to validate typing in runtime as well 
for some use cases
:::

:::{.callout-tip}
Check MyPy usage creating a adding function: 

```python
def add(a: int, b: int) -> int:
  return a + b
```

and testing the function using:

```python
add('1', '2')
```
:::

## Flake8 {.smaller}

> Is a style guide enforcement

It saves time when team start discussing empty spaces and small code style details on pull requests

Run the following snippet in your terminal:

```bash
! nbqa flake8 *.ipynb
```

:::{.callout-tip}
- Test flake8 usage by adding a import that it's not used like `import matplotlib`
- Check the .flake8 file (it is used by autopep8* as well)
:::

## Autopep8

> Code format enforcer according to [PEP 8](https://peps.python.org/pep-0008/)

Is very useful to clean empty space and format notebooks with a well known code style

Run the following snippet in your terminal:

```bash
! nbqa autopep8 nbs/*.ipynb --in-place
```

## Black

> Another code format enforcer but more flexible

Works similar to autopep8 but not restrinted to PEP 8

Run the following snippet in your terminal:

```bash
! nbqa black nbs/*.ipynb
```

# Advanced testing

Test isolation with i(pytest). Let's explore the advanced testing nb

# Publishing

Nbdev allows publish package by using its CLI

## Conda and PyPI

- It can publish on both: `nbdev_pypi`, `nbdev_conda`, `nbdev_release_both`
- [Less documented](https://github.com/fastai/nbdev/pull/1320) but it also allows Test PyPI usage (`nbdev_pypi --repository testpypi`)

:::{.callout-note}
As we did before with the `{username}_package` let's publish this package (`mydeck`) using `nbdev_pypi --repository testpypi`
:::

## Secret keys

Python package system uses the [.pypirc](https://packaging.python.org/en/latest/specifications/pypirc/) to define the repository packages:

```toml
[distutils]
index-servers = testpypi

[testpypi]
repository=https://test.pypi.org/legacy/
username=<your-username>
password=<your-password>
```

## Secret keys

Username and password usage are not encouraged, consider gerate a token for your package:

```toml
[distutils]
index-servers = testpypi

[testpypi]
repository=https://test.pypi.org/legacy/
username = __token__
password = <PyPI token>
```

## Custom repositories {.smaller}

To add custom repositories (private or not) you can change the `~/.pypirc` and add the repository name to `[distutils]`

```toml
[distutils]
index-servers =
    pypi
    testpypi
    private-repository

[pypi]
username = __token__
password = <PyPI token>

[testpypi]
username = __token__
password = <TestPyPI token>

[private-repository]
repository = <private-repository URL>
username = <private-repository username>
password = <private-repository password>
```

# CI/CD

Let's run QA and publish our package using Github Actions

## Continuous Integration/Continuous Delivery 

- Well know concept in the DevOps area that became more acessible to developers recently
- Method to frequently deliver apps to customers
- It automates some stages of the app development
- Enforce security of the deliver

## Continuous Integration/Continuous Delivery 

```{mermaid}
flowchart LR
  subgraph CI
    A["Build"] --> B["Test"] --> C["Merge"]
  end
  
  subgraph CD
      D["Automatically release"] --> E["Automatically deploy"]
  end
  
  C --> D
```

## Tools

- [Github Actions](https://github.com/features/actions)
- [Gitlab CI/CD](https://about.gitlab.com/)
- [Jenkins](https://www.jenkins.io/)
- [Circle CI](https://circleci.com/)

And many others

## Github Actions

- Easy to use
- Free until 2000 minutes/month
- Most tools has some integration with it

## Github actions core concepts {.smaller}

- *Events*: It's a specific activity in your Github repository that triggers an action. For example, the opening of a PR, a commit been sent, etc
- *Jobs*: It's a sequence of steps that will be executed by a shell script or action. Jobs can be executed in parallel or sequentially
- *Action*: custom application that uses the Github Action platform, it usually automates a repetitive task like the configuration of an environment or managing a complex dependency, even service authentication (like cloud)
- *Runner*: Server that executes workflows. Every runner executes a job per time

## Github secrets & Test Pypi token {.smaller}

- Let's create a token for our mydeck repository

:::{.callout-note}
At Test PyPI webpage access `Account Settings >> Api Tokens >> Add Api Token`
:::

- In your Github repository go to: `Settings >> Actions (at security tab) >> New repository secret` and add the token with the name `TEST_PYPI_API_TOKEN`

## Workflows

(Interactive explation of files from `.github/worflows/*.yaml`)

## Publising using CI

:::{.callout-note}
- Upgrade the version at `settings.ini`
- Update your changelog
- Commit and push your changes to Github
- Check if tests and lint are passing
- Create a new release at github following the semantic versioning to use our CD of the package
:::

# Python Packaging history

Let's look at the past to discuss the future of Python packaging

## History 

::: {.incremental}
- Python 1 (1998-2000) didn't have a package manager
- Distutils was added to Python 1.6 using setup.py
- In 2003 setuptools was introduced as an improvement of distutils
- In 2004 easy_install was developed to be used alongside setuptools
- In 2008 the PyPA (Python Packaging Authority) was founded 
:::

## History

::: {.incremental}
- In 2011 `pip` was became the default package manager
- In 2013 the `wheel` package was introduced
- In 2017 the `flit` package was developed, introducing `pyproject.toml`
- In 2020 the PEP 621 made `pyproject.toml` the standard way to develop packages
:::

# Quarto and Styling

[Quarto](quarto.org/) is an open source tool that allows creating content dinamically using Python, R, Julia and Observable. It was introduced to Nbdev recently

## Executing

Try to execute `quarto preview nbs/00_deck.ipynb` in the terminal

:::{.callout-note}
Quarto is already installed on our setup thanks to `nbdev_install_quarto`
:::

## Doc preview

Change ports when rendering nbdev docs can be annoying, change `_quarto.yml` to fix a port and avoid opening new tabs.

```yaml
project:
  preview:
    port: 3000
    browser: false
```

## Doc navigation

Quarto allows easy customization of the navbar

```yaml
website:
  navbar:
    background: primary
      search: true
      collapse-below: lg
      left:
        - text: "My page"
          href: gettings_started.ipynb
      right:
        - icon: github
          href: "https://github.com/user/project"
```

## Google analytics

Activate analytics tracking but remember asking for cookie consent if your country legislation requires it

```yaml
website:
  google-analytics: "UA-XXXXXXXX"
  cookie-consent: true
```

## Dark mode

You can define the quarto theme to (de)activate dark mode

```yaml
format:
  html:
    theme:
      light: flatly
      dark: darkly
```

## Page navigation

Your project may require continuous page navigation

```yaml
website:
  page-navigation: true
```

## Reader mode

Enable reader mode

```yaml
website:
  reader-mode: true
```

# Playgroud

We already published our package and know more about nbdev powers, let's have some fun with nbs

## Ipywidgets

Let's run the UI for jupyter notebooks nb

:::{.callout-tip}
Run voila over the nb
:::

## Chat GPT

Let's use Chat GPT in our nbs by running the chat gpt nb

# Conclusion

